{% extends 'base.html' %}

{% block body %}
<h2 class="notes">Perceptron Primer</h2>
<img src="../static/perceptron_primer.svg" class="center-image">
<p class="notes">
    The perceptron was first introduced by Frank Rosenblatt in 1957 and serves as the building block of neural networks. In short, a perceptron is a linear classifier. This means that the perceptron analyzes input data and then returns either a 1 or a 0. Lets quickly walk through how a perceptron works.
</p>
<h3 class="notes">Step 1: Multiply Inputs By Their Corresponding Weights</h3>
<p class="notes">
    Any number of variables can be submitted as inputs to a perceptron and each of these variables is multiplied by a corresponding weight. Initially, the weights can be manually or randomly generated. However, modern neural networks include a backpropagation algorithm that analyzes the perceptron's error and then adjusts the weights to decrease the error.
</p>
<img src="../static/perceptron_primer_s1.png" class="center-image-png" style="width: 7%">
<h3 class="notes">Step 2: Sum the Weighted Inputs</h3>
<p class="notes">
    The weighted inputs are then added together.
</p>
<img src="../static/perceptron_primer_s2.png" class="center-image-png" style="width: 15%">
<h3 class="notes">Step 3: Evaluate the Weighted Sum Against the Activation Function</h3>
<p class="notes">
    The activation function maps the weighted sum to the perceptron's resulting values. There are nine different activation functions that can be used to map weighted sums to resulting values. For simplicity, lets consider the binary step activation function:
</p>
<img src="../static/binary_step.png" class="center-image-png" style="width: 25%">
<p class="notes">
    When using the binary step activation function, if the weighted sum is greater than 0, the function returns a value of 1. Conversely, if the weighted sum is equal to or less than 0, the function returns a value of 0. Below is a table of other activation functions that can be used in perceptrons.
</p>
<img src="../static/activation_functions.png-png" class="center-image" style="width: 60%">
<h3 class="notes">Example</h3>
<p class="notes">
    Let's solidify the above with an example. Consider a scenario in which we're trying to predict whether a student will be admitted to a University or not. We have a dataset containing the following three variables, their grades and SAT scores on a scale of 1 to 10 and whether or not they were admitted into the University. Initial analysis indicates that the linear boundary, <i>x + y - 15 = 0</i>, provides a good indication of whether a student was accepted into the university; those whose weighted sum is greater than or equal to 0 are accepted and those whose weighted sum is less than zero are rejected.
</p>
<img src="../static/perceptron_primer_example_1.svg" class="center-image" style="width: 60%">
<p class="notes">
    Our perceptron is simply the encoding of the above equation. The inputs to the perceptron are the students grades and test scores. The weights associated with the grades and test scores are both 1. The weight associated with the bias is -15. These equate to our linear boundary which is represented by the node containing our graph. Finally, we use a binary step activation function to map our weighted sum to a resulting value of either 0 or 1. We can see that a student with grade and test scores of 7 and 9 respectively would have a weighted sum of 1. Our binary step activation function would thus return a value of 1, indicating the student would be admitted.
</p>
<img src="../static/perceptron_primer_example_2.svg" class="center-image" style="width: 60%">
<p class="notes">
    This is an example of a single-layer perceptron and is only capable of learning linearly separable patterns (think y=mx+b). Models with two or more layers of perceptrons are known as feedforward neural networks. These multilayer perceptrons are capable of evaluating non-linear separable patterns; making them capable of finding patterns or insights simple single-layer perceptrons are incapable of finding.
</p>
{% endblock %}