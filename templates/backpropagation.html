{% extends 'base.html' %}
{% block head %}
<meta property=”og:title” content=”Backpropagation” />
<meta name=”image” property=”og:image” content=”%static%/images/backpropagation/bp_base.svg” />
<meta name=”author” content=”Thomas Wileman” />
<meta property=”og:description” content=”Note on backpropagation“ />
<meta property=”og:url” content=”https://thomaswileman.com/backpropagation" />
{% endblock %}
{% block body %}
<h2 class="notes">
    Backpropagation
</h2>
<p class="notes">
    Backpropagation is the process of adjusting a model's weights based on its error. The intention of backpropagation is to adjust the model so that the output of the next model run (feedforward operation) is more closely aligned to the desired outcome that the previous model run. This process is completed via the following steps:
</p>
<div class="note-container">
    <ul>
        <li>
            Complete one run of the model (feedforward operation).
        </li>
        <li>
            Calculate the model's error by examining the difference between the model's output and the desired output.
        </li>
        <li>
            Use the error and a learning rate to tune the model by changing the weights.
        </li>
        <li>
            Repeat until the model is reliable.
        </li>
    </ul>
</div>
<p class="notes">
    Let's run through a simple example and examine how new model weights are calculated in backpropagation. Below is our initial model. It has two inputs, one hidden layer, and two outputs. The activation function in both the hidden layer and output layer is a sigmoid function. Our deisred outputs are 0.01 and 0.99.
</p>
<img src="../static/images/backpropagation/bp_base.svg" class="center-image">
<h3 class="notes">
    Step 1: Feedforward
</h3>
<p class="notes">
    We be begin by initializing our model with random weights (you can also manually input initial weights if you want) and then calculating the model's outputs. See a graphical representation of this step and each calculation below.
</p>
<img src="../static/images/backpropagation/bp_initial.svg" class="center-image">
<div class="equation-container">
    <div class="equation-container" style="float:left; width: 50%;">
        <div class="equation-container">
            $net_{h_1} = w_{1}*i_{1}+w_{2}*i_{2}+b_{1}*1$
        </div>
        <div class="equation-container">
            $net_{h_1} = 0.10*0.10+0.25*0..10+0.45*1$
        </div>
        <div class="equation-container">
            $net_{h_1} = 0.49$
        </div>
        <div class="equation-container">
            $out_{h_1} = \frac{1}{1+e^{-net_h1}} = \frac{1}{1+e^{-0.46}} = 0.62$
        </div>
        <hr>
        <div class="equation-container">
            $net_{h_2} = w_{3}*i_{1}+w_{4}*i_{2}+b_{1}*1$
        </div>
        <div class="equation-container">
            $net_{h_2} = 0.25*0.20+0.40*0.20+0.45*1$
        </div>
        <div class="equation-container">
            $net_{h_2} = 0..58$
        </div>
        <div class="equation-container">
            $out_{h_2} = \frac{1}{1+e^{-net_h2}} = \frac{1}{1+e^{-0.48}} = 0.64$
        </div>
    </div>
    <div class="equation-container" style="float:right; width: 50%;">
        <div class="equation-container">
            $net_{o_1} = w_{5}*h_{1}+w_{6}*h_{2}+b_{1}*1$
        </div>
        <div class="equation-container">
            $net_{o_1} = 0.30*0.62+0.35*0.62+0.35*1$
        </div>
        <div class="equation-container">
            $net_{o_1} = 0.75$
        </div>
        <div class="equation-container">
            $out_{o_1} = \frac{1}{1+e^{-net_o1}} = \frac{1}{1+e^{-0.68}} = 0.68$
        </div>
        <hr>
        <div class="equation-container">
            $net_{o_2} = w_{7}*o_{1}+w_{8}*h_{2}+b_{2}*1$
        </div>
        <div class="equation-container">
            $net{o_2} = 0.50*0.64+0.55*0.64+0.35*1$
        </div>
        <div class="equation-container">
            $net_{o_2} = 1.02$
        </div>
        <div class="equation-container">
            $out_{o_2} = \frac{1}{1+e^{-net_o1}} = \frac{1}{1+e^{-1.02}} = 0.74$
        </div>
    </div>
</div>
<p class="notes">
    -
</p>
<p class="notes">
    We can see that the current ouputs of our model are not close to our desired outputs.
</p>
<h3 class="notes">
    Step 2: Calculate Model Error
</h3>
<div class="equation-container">
    <div class="equation-container">
        <div class="equation-container">
            $E_{total} = \sum\frac{1}{2}(target-output)^{2}$
        </div>
    </div>
</div>
<p class="notes">
    Next, we calculate the model's total error. This is done using the squared error function (see the formula above). In the following steps, we'll calculate the individual errors for each output and the sum them together to get the model's total error.
</p>
<div class="equation-container">
    <div>
        $E_{01}=\sum\frac{1}{2}(target-output)^{2}=\sum\frac{1}{2}(0.01-0.68)^{2}=.22$
    </div>
    <div>
        $E_{02}=\sum\frac{1}{2}(target-output)^{2}=\sum\frac{1}{2}(0.99-0.74)^{2}=.03$
    </div>
    <div>
        $E_{total}=E_{01}+E_{02}=.22+.03=.26$
    </div>
</div>
<p class="notes">
    Now that we have the total error we can adjust the model's weights to improve its performance.
</p>
<h3 class="notes">
    Step 3: Tune Model
</h3>
<p class="notes">
    When we tune a model we adjust each individual weight so that when the model is run again, the new output is closer to the desired output than the previous output; minimizing our error. To do this we need to calculate how much a change in $w_{5}$ impacts the total error. This is done using partial derivatives. Partial derivatives allow us to see how much a function changes when one variable is allowed to change and all others are held constant; providing us a sense of how much one variable impacts a function. The impact of $w_{5}$ on the total error can be represented by the partial derivative of $E_{total}$ with respect to $w_{5}$, $\frac{\partialE_{total}}{\partialw_{5}}$.
</p>
<p class="notes">
    We can not calculate $\frac{\partialE_{total}}{\partialw_{5}}$ directly since $w_{5}$ is not an input into the error function used above. $w_{5}$ is an input into a preceding calculation used to calculate total error making the total error function a composite function. However, we can use the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a> to calculate $\frac{\partialE_{total}}{\partialw_{5}}$. The chain rule allows us to calculate the impact of a variables in a composite function by multiplying linking partial derivatives together. See the visualization below:
</p>
<img src="../static/images/backpropagation/bp_part_derv.svg" class="center-image">
<p class="notes">
    When we chain the three partial derivatives identified in the image above we can see how they cancel out to provide the partial derivative we need.
</p>
<div class="center-image">
    $\frac{\partialE_{total}}{\partialw_{5}}=\frac{\partialE_{total}}{\partialout_{o1}}x\frac{\partialout_{o1}}{\partialnet_{o1}}x\frac{\partialnet_{o1}}{\partialw_{5}}$
</div>
<p class="notes">
    Now we need to find the answer for each partial derivative in our chain to generate to get our answer. We'll start by calculating $\frac{\partialE_{total}}{\partialout_{o1}}$.
</p>
<div class="equation-container">
    <div>
        $E_{total}=\frac{1}{2}(target_{o1}-output_{o1})^{2}+\frac{1}{2}(target_{o2}-output_{o2})^{2}$
    </div>
    <div>
        $\frac{\partialE_{total}}{\partialout_{o1}}=2*\frac{1}{2}(target_{o1}-output_{o1})^{2-1}*-1+0$
    </div>
    <div>
        $\frac{\partialE_{total}}{\partialout_{o1}}=-(target_{o1}-output_{o1})=-(.01-.22)=.21$
    </div>
</div>
<p class="notes">
    Next, we'll calculate the derivative of our logistic activation function with respect to the net input, $\frac{\partialout_{o1}}{\partialnet_{o1}}$.
</p>
<div class="equation-container">
    <div>
        $out_{o_1} = \frac{1}{1+e^{-net_o1}}$
    </div>
    <div>
        $\frac{\partialout_{o1}}{\partialnet_{o1}}=out_{o1}(1-out_{o1})$
    </div>
    <div>
        $\frac{\partialout_{o1}}{\partialnet_{o1}}=.22(1-.22)=.17$
    </div>
</div>
<p class="notes">
    Then we calculate the derivative of the net input with respect to $w_{5}$, $\frac{\partialnet_{o1}}{\partialw_{5}}$.
</p>
<div class="equation-container">
    <div>
        $net_{o_1} = w_{5}*out_{h_1}+w_{6}*out_{h_2}+b_{2}*1$
    </div>
    <div>
        $\frac{\partialnet_{o1}}{\partialw_{5}}=1*out_{h_1}*w_{5}^{1-1}+0+0$
    </div>
    <div>
        $\frac{\partialnet_{o1}}{\partialw_{5}}=out_{h_1}=.74$
    </div>
</div>
<p class="notes">
    Now we can combine the results of the three partial derivatives we've calculated to get the answer for $\frac{\partialE_{total}}{\partialw_{5}}$.
</p>
<div class="equation-container">
    <div>
        $\frac{\partialE_{total}}{\partialw_{5}}=\frac{\partialE_{total}}{\partialout_{o1}}x\frac{\partialout_{o1}}{\partialnet_{o1}}x\frac{\partialnet_{o1}}{\partialw_{5}}$
    </div>
    <div>
        $\frac{\partialE_{total}}{\partialw_{5}}=.21*.17*.75=0.03
    </div>
</div>
<p class="notes">
    Finally, we multiply the value we have calculated by a learning rate (we will use 0.2) and subtract that value from the current weight.
</p>
<div class="equation-container">
    <div>
        $w_{5}^{+}=w_{5}-\eta*\frac{\partialnet_{o1}}{\partialw_{5}}$
    </div>
    <div>
        $w_{5}^{+}=.3-(.2+.03)=0.29
    </div>
</div>
<p class="notes">
    Applying this same methodology to the remaining weights we end up with the following:
</p>
<div class="equation-container">
    <div>$w_{6}^{+}=0.34$</div>
    <div>$w_{7}^{+}=0.51$</div>
    <div>$w_{8}^{+}=0.56$</div>
    <div>$w_{1}^{+}=0.10$</div>
    <div>$w_{2}^{+}=0.25$</div>
    <div>$w_{3}^{+}=0.25$</div>
    <div>$w_{4}^{+}=0.40$</div>
</div>
<h3 class="notes">
    Step 4: Repeat
</h3>
<p class="notes">
    Now, we replace our model's weights with the updated values and start again.
</p>
<img src="../static/images/backpropagation/bp_final.svg" class="center-image">
<p class="notes">
    Notice that while our total error after one step will still be 0.26, the net outputs in the hidden layer have changed. Given enough iterations, these small steps will result in large changes to the weights of our model, reducing the model's total error and improving our predictive capability.
</p>
{% endblock %}